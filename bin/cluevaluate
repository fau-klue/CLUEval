#!/usr/bin/env python3

import argparse

from clueval.evaluation import evaluate
from clueval.version import __version__


def arguments():
    parser = argparse.ArgumentParser(
        description="cluevaluate: evaluate span predictions",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument("-v", "--version", action="version", version="clueval %s" % __version__,
                        help="output version information and exit")

    parser.add_argument("reference", help="Path to reference file.")
    parser.add_argument("candidate", help="Path to candidate or prediction file.")
    parser.add_argument("-a", "--annotation_layers", nargs="+", type=str, default=None, help="Input names for annotation layers.")
    parser.add_argument("-d", "--domain_column", type=int, default=None, help="Column ID for domain information.")
    parser.add_argument("-fc", "--filter_column", type=str, default=None, help="Column name for filtering.")
    parser.add_argument("-fv", "--filter_value", type=str, default=None, help="Filter column by value.")
    parser.add_argument("-ce", "--categorical_eval", action="store_true", help="Compute metrics for each category.")
    return parser.parse_args()


if __name__ == "__main__":

    args = arguments()

    df = evaluate(args.reference,
                  args.candidate,
                  annotation_layer=args.annotation_layers,
                  domain_column=args.domain_column,
                  filter_column=args.filter_column,
                  filter_value=args.filter_value,
                  categorical_evaluation=args.categorical_eval)

    print(df.to_csv(sep='\t', index=False))
